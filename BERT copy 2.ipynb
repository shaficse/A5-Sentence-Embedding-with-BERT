{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bSz5jzj61nHc"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "from random import *\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"nuvocare/WikiMedical_sentence_similarity\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New Train dataset size: 85\n",
            "New Test dataset size: 15\n"
          ]
        }
      ],
      "source": [
        "# Define the split sizes\n",
        "train_size = 85\n",
        "test_size = 15  # Explicit for clarity, though it's simply 100 - 85\n",
        "\n",
        "# Apply the split to both 'train' and 'test' sets in the DatasetDict\n",
        "new_train = dataset['train'].select(range(train_size))\n",
        "new_test = dataset['train'].select(range(train_size, train_size + test_size))\n",
        "\n",
        "# Additionally, if you wish to apply the split to the original 'test' dataset as well, you can do:\n",
        "# For clarity, showing how to reassign within the DatasetDict, but typically, you adjust only the 'train' part for such operations\n",
        "dataset['train'] = new_train\n",
        "dataset['test'] = dataset['test'].select(range(test_size))\n",
        "\n",
        "# Confirming the new sizes\n",
        "print(f\"New Train dataset size: {len(dataset['train'])}\")\n",
        "print(f\"New Test dataset size: {len(dataset['test'])}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text1', 'text2', 'label'],\n",
              "        num_rows: 85\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text1', 'text2', 'label'],\n",
              "        num_rows: 15\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 263,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocessing\n",
        "\n",
        "### Tokenization and numericalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformed Train dataset size: 85\n",
            "Transformed Test dataset size: 15\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "\n",
        "# Define your transformation function\n",
        "def transform_data(example):\n",
        "    text1 = re.sub(\"[.,!?\\\\-]\", '', example['text1'].lower())\n",
        "    text2 = re.sub(\"[.,!?\\\\-]\", '', example['text2'].lower())\n",
        "    label = example['label']\n",
        "\n",
        "    if np.random.random() < 0.5:\n",
        "        return {'text1': text1, 'text2': text2, 'isNext': label == 1}  # Assuming 1 indicates similarity\n",
        "    else:\n",
        "        return {'text1': text2, 'text2': text1, 'isNext': label == 1}  # Flip text1 and text2\n",
        "\n",
        "# Assuming 'dataset' is your DatasetDict\n",
        "# Apply the transformation directly to each split in the DatasetDict\n",
        "transformed_dataset = {\n",
        "    split: dataset[split].map(transform_data, remove_columns=['text1', 'text2', 'label'])\n",
        "    for split in dataset.keys()\n",
        "}\n",
        "\n",
        "# Now, 'transformed_dataset' should be correctly transformed\n",
        "print(f\"Transformed Train dataset size: {len(transformed_dataset['train'])}\")\n",
        "print(f\"Transformed Test dataset size: {len(transformed_dataset['test'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'train': Dataset({\n",
              "     features: ['text1', 'text2', 'isNext'],\n",
              "     num_rows: 85\n",
              " }),\n",
              " 'test': Dataset({\n",
              "     features: ['text1', 'text2', 'isNext'],\n",
              "     num_rows: 15\n",
              " })}"
            ]
          },
          "execution_count": 265,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformed_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 85/85 [00:00<00:00, 397.52 examples/s]\n",
            "Map: 100%|██████████| 15/15 [00:00<00:00, 534.06 examples/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "import torchtext\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Load the 'basic_english' tokenizer\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "# Adjust the tokenize_data function to handle batch processing\n",
        "def tokenize_data(batch, tokenizer):\n",
        "    # Tokenize each text in the batch for text1 and text2\n",
        "    # text1_tokens = [tokenizer(text) for text in batch['text1']]\n",
        "    # text2_tokens = [tokenizer(text) for text in batch['text2']]\n",
        "    \n",
        "    text1_tokens = [tokenizer(re.sub(\"[.,!?\\\\-]\", '', text.lower())) for text in batch['text1']]\n",
        "    text2_tokens = [tokenizer(re.sub(\"[.,!?\\\\-]\", '', text.lower())) for text in batch['text2']]\n",
        "\n",
        "\n",
        "    # Return a dictionary with the tokenized texts\n",
        "    return {'text1_tokens': text1_tokens, 'text2_tokens': text2_tokens}\n",
        "\n",
        "# Apply tokenization to each split in your transformed dataset\n",
        "tokenized_dataset = {\n",
        "    split: transformed_dataset[split].map(lambda batch: tokenize_data(batch, tokenizer), batched=True, remove_columns=['text1', 'text2'])\n",
        "    for split in transformed_dataset.keys()\n",
        "}\n",
        "\n",
        "# Proceed with building the vocab\n",
        "special_tokens = ['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n",
        "\n",
        "# Since build_vocab_from_iterator expects a flat iterator of tokens, you need to flatten the token lists\n",
        "def flat_map(tokens_lists):\n",
        "    for tokens in tokens_lists:\n",
        "        for token in tokens:\n",
        "            yield token\n",
        "\n",
        "# Adjust the call to build_vocab_from_iterator\n",
        "vocab = build_vocab_from_iterator(\n",
        "    flat_map(tokenized_dataset['train']['text1_tokens'] + tokenized_dataset['train']['text2_tokens']),\n",
        "    specials=special_tokens\n",
        ")\n",
        "vocab.set_default_index(vocab['[UNK]'])\n",
        "\n",
        "# Save vocab for later use\n",
        "torch.save(vocab, './model/vocab.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of tokens in vocab: 93\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total number of tokens in vocab: {len(vocab)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index of '[PAD]': 0\n",
            "Index of '[CLS]': 1\n",
            "Index of '[SEP]': 2\n",
            "Index of '[MASK]': 3\n",
            "Index of '[UNK]': 4\n",
            "Index of 'the': 4\n",
            "Index of 'of': 4\n",
            "Index of 'and': 4\n"
          ]
        }
      ],
      "source": [
        "tokens_to_check = ['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]', 'the', 'of', 'and']\n",
        "for token in tokens_to_check:\n",
        "    print(f\"Index of '{token}': {vocab[token]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'isNext': False,\n",
              " 'text1_tokens': ['econometric',\n",
              "  'studies',\n",
              "  'have',\n",
              "  'shown',\n",
              "  'that',\n",
              "  'this',\n",
              "  'effect',\n",
              "  'cannot',\n",
              "  'be',\n",
              "  'explained',\n",
              "  'by',\n",
              "  'a',\n",
              "  'variety',\n",
              "  'of',\n",
              "  'alternative',\n",
              "  'factors',\n",
              "  'including',\n",
              "  'differential',\n",
              "  'trends',\n",
              "  'across',\n",
              "  'areas',\n",
              "  'changing',\n",
              "  'crop',\n",
              "  'prices',\n",
              "  'shifts',\n",
              "  'in',\n",
              "  'certain',\n",
              "  'educational',\n",
              "  'and',\n",
              "  'health',\n",
              "  'policies',\n",
              "  'and',\n",
              "  'the',\n",
              "  'effect',\n",
              "  'of',\n",
              "  'malaria',\n",
              "  'eradication',\n",
              "  'no',\n",
              "  'significant',\n",
              "  'contemporaneous',\n",
              "  'results',\n",
              "  'were',\n",
              "  'found',\n",
              "  'for',\n",
              "  'adults',\n",
              "  'who',\n",
              "  'should',\n",
              "  'have',\n",
              "  'benefited',\n",
              "  'less',\n",
              "  'from',\n",
              "  'the',\n",
              "  'intervention',\n",
              "  'owing',\n",
              "  'to',\n",
              "  'their',\n",
              "  'substantially',\n",
              "  'lower',\n",
              "  '(',\n",
              "  'prior',\n",
              "  ')',\n",
              "  'infection',\n",
              "  'rates',\n",
              "  'the',\n",
              "  'program',\n",
              "  'nearly',\n",
              "  'eradicated',\n",
              "  'hookworm',\n",
              "  'and',\n",
              "  'would',\n",
              "  'flourish',\n",
              "  'afterward',\n",
              "  'with',\n",
              "  'new',\n",
              "  'funding',\n",
              "  'as',\n",
              "  'the',\n",
              "  'rockefeller',\n",
              "  'foundation',\n",
              "  'international',\n",
              "  'health',\n",
              "  'divisionthe',\n",
              "  'rfs',\n",
              "  'hookworm',\n",
              "  'campaign',\n",
              "  'in',\n",
              "  'mexico',\n",
              "  'showed',\n",
              "  'how',\n",
              "  'science',\n",
              "  'and',\n",
              "  'politics',\n",
              "  'play',\n",
              "  'a',\n",
              "  'role',\n",
              "  'in',\n",
              "  'developing',\n",
              "  'health',\n",
              "  'policies',\n",
              "  'it',\n",
              "  'brought',\n",
              "  'together',\n",
              "  'government',\n",
              "  'officials',\n",
              "  'health',\n",
              "  'officials',\n",
              "  'public',\n",
              "  'health',\n",
              "  'workers',\n",
              "  'rockefeller',\n",
              "  'officials',\n",
              "  'and',\n",
              "  'the',\n",
              "  'community',\n",
              "  'this',\n",
              "  'campaign',\n",
              "  'was',\n",
              "  'launched',\n",
              "  'to',\n",
              "  'eradicate',\n",
              "  'hookworms',\n",
              "  'in',\n",
              "  'mexico',\n",
              "  'although',\n",
              "  'the',\n",
              "  'campaign',\n",
              "  'did',\n",
              "  'not',\n",
              "  'focus',\n",
              "  'on',\n",
              "  'longterm',\n",
              "  'treatments',\n",
              "  'it',\n",
              "  'did',\n",
              "  'set',\n",
              "  'the',\n",
              "  'terms',\n",
              "  'of',\n",
              "  'the',\n",
              "  'relationship',\n",
              "  'between',\n",
              "  'mexico',\n",
              "  'and',\n",
              "  'the',\n",
              "  'rockefeller',\n",
              "  'foundation',\n",
              "  'the',\n",
              "  'scientific',\n",
              "  'knowledge',\n",
              "  'behind',\n",
              "  'this',\n",
              "  'campaign',\n",
              "  'helped',\n",
              "  'shape',\n",
              "  'public',\n",
              "  'health',\n",
              "  'policies',\n",
              "  'improved',\n",
              "  'public',\n",
              "  'health',\n",
              "  'and',\n",
              "  'built',\n",
              "  'a',\n",
              "  'strong',\n",
              "  'relationship',\n",
              "  'between',\n",
              "  'us',\n",
              "  'and',\n",
              "  'mexicoin',\n",
              "  'the',\n",
              "  '1920s',\n",
              "  'hookworm',\n",
              "  'eradication',\n",
              "  'reached',\n",
              "  'the',\n",
              "  'caribbean',\n",
              "  'and',\n",
              "  'latin',\n",
              "  'america',\n",
              "  'where',\n",
              "  'great',\n",
              "  'mortality',\n",
              "  'was',\n",
              "  'reported',\n",
              "  'among',\n",
              "  'people',\n",
              "  'in',\n",
              "  'the',\n",
              "  'west',\n",
              "  'indies',\n",
              "  'towards',\n",
              "  'the',\n",
              "  'end',\n",
              "  'of',\n",
              "  'the',\n",
              "  '18th',\n",
              "  'century',\n",
              "  'as',\n",
              "  'well',\n",
              "  'as',\n",
              "  'through',\n",
              "  'descriptions',\n",
              "  'sent',\n",
              "  'from',\n",
              "  'brazil',\n",
              "  'and',\n",
              "  'various',\n",
              "  'other',\n",
              "  'tropical',\n",
              "  'and',\n",
              "  'subtropical',\n",
              "  'regions',\n",
              "  'treatments',\n",
              "  'treatment',\n",
              "  'in',\n",
              "  'the',\n",
              "  'early',\n",
              "  '20th',\n",
              "  'century',\n",
              "  'relied',\n",
              "  'on',\n",
              "  'the',\n",
              "  'use',\n",
              "  'of',\n",
              "  'epsom',\n",
              "  'salt',\n",
              "  'to',\n",
              "  'reduce',\n",
              "  'protective',\n",
              "  'mucus',\n",
              "  'followed',\n",
              "  'by',\n",
              "  'thymol',\n",
              "  'to',\n",
              "  'kill',\n",
              "  'the',\n",
              "  'worms',\n",
              "  'by',\n",
              "  'the',\n",
              "  '1940s',\n",
              "  'tetrachloroethylene',\n",
              "  'was',\n",
              "  'the',\n",
              "  'leading',\n",
              "  'method'],\n",
              " 'text2_tokens': ['spinraza',\n",
              "  'access',\n",
              "  'by',\n",
              "  'country',\n",
              "  'treatsma',\n",
              "  '18',\n",
              "  'october',\n",
              "  '2018',\n",
              "  'retrieved',\n",
              "  '20190528']}"
            ]
          },
          "execution_count": 269,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset['train'][0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data loader\n",
        "\n",
        "We gonna make dataloader.  Inside here, we need to make two types of embeddings: **token embedding** and **segment embedding**\n",
        "\n",
        "1. **Token embedding** - Given “The cat is walking. The dog is barking”, we add [CLS] and [SEP] >> “[CLS] the cat is walking [SEP] the dog is barking”. \n",
        "\n",
        "2. **Segment embedding**\n",
        "A segment embedding separates two sentences, i.e., [0 0 0 0 1 1 1 1 ]\n",
        "\n",
        "3. **Masking**\n",
        "As mentioned in the original paper, BERT randomly assigns masks to 15% of the sequence. In this 15%, 80% is replaced with masks, while 10% is replaced with random tokens, and the rest 10% is left as is.  Here we specified `max_pred` \n",
        "\n",
        "4. **Padding**\n",
        "Once we mask, we will add padding. For simplicity, here we padded until some specified `max_len`. \n",
        "\n",
        "Note:  `positive` and `negative` are just simply counts to keep track of the batch size.  `positive` refers to two sentences that are really next to one another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/85 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 85/85 [00:00<00:00, 434.18it/s]\n",
            "100%|██████████| 15/15 [00:00<00:00, 547.44it/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Assuming vocab and transformed_dataset are already defined and loaded\n",
        "\n",
        "def make_batch(dataset, vocab, max_mask, max_len):\n",
        "    data = []\n",
        "    \n",
        "    for example in tqdm(dataset):\n",
        "        # Transform the list of tokens to list of token indices based on vocab\n",
        "        tokens_a = [vocab[token] if token in vocab else vocab['[UNK]'] for token in example['text1_tokens']]\n",
        "        tokens_b = [vocab[token] if token in vocab else vocab['[UNK]'] for token in example['text2_tokens']]\n",
        "\n",
        "        # 1. Token embedding - add CLS and SEP\n",
        "        input_ids = [vocab['[CLS]']] + tokens_a + [vocab['[SEP]']] + tokens_b + [vocab['[SEP]']]\n",
        "\n",
        "        # 2. Segment embedding - which sentence is 0 and 1\n",
        "        segment_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        # Ensure length does not exceed max_len\n",
        "        input_ids = input_ids[:max_len]\n",
        "        segment_ids = segment_ids[:max_len]\n",
        "\n",
        "        # 3. Predicted mask positions\n",
        "        n_pred = min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
        "        candidates_masked_pos = [i for i, token in enumerate(input_ids) if token != vocab['[CLS]'] and token != vocab['[SEP]']]\n",
        "        np.random.shuffle(candidates_masked_pos)\n",
        "        masked_tokens, masked_pos = [], []\n",
        "\n",
        "        for pos in candidates_masked_pos[:n_pred]:\n",
        "            masked_pos.append(pos)\n",
        "            masked_tokens.append(input_ids[pos])\n",
        "            rand_val = np.random.random()\n",
        "            if rand_val < 0.1:\n",
        "                input_ids[pos] = vocab[vocab.get_itos()[np.random.randint(5, len(vocab) - 1)]]\n",
        "            elif rand_val < 0.8:\n",
        "                input_ids[pos] = vocab['[MASK]']\n",
        "\n",
        "        # 4. Pad the sentences and masks to max_len\n",
        "        n_pad = max_len - len(input_ids)\n",
        "        input_ids.extend([vocab['[PAD]']] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "\n",
        "        n_pad = max_mask - len(masked_tokens)\n",
        "        masked_tokens.extend([0] * n_pad)\n",
        "        masked_pos.extend([0] * n_pad)\n",
        "\n",
        "        instance = [input_ids, segment_ids, masked_tokens, masked_pos, [int(example['isNext'])]]\n",
        "        instance = [torch.LongTensor(i) for i in instance]\n",
        "        data.append(instance)\n",
        "        \n",
        "    return data\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    input_ids = torch.stack([item[0] for item in batch])\n",
        "    segment_ids = torch.stack([item[1] for item in batch])\n",
        "    masked_tokens = torch.stack([item[2] for item in batch])\n",
        "    masked_pos = torch.stack([item[3] for item in batch])\n",
        "    isNext = torch.stack([item[4] for item in batch])\n",
        "    return input_ids, segment_ids, masked_tokens, masked_pos, isNext\n",
        "\n",
        "# Parameters\n",
        "batch_size = 16\n",
        "max_mask = 5\n",
        "max_len = 512\n",
        "\n",
        "# Prepare the data\n",
        "train_data = make_batch(tokenized_dataset['train'], vocab, max_mask, max_len)\n",
        "val_data = make_batch(tokenized_dataset['test'], vocab, max_mask, max_len)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, collate_fn=custom_collate_fn)\n",
        "\n",
        "# Example usage\n",
        "for input_ids, segment_ids, masked_tokens, masked_pos, isNext in train_loader:\n",
        "    break  # This just demonstrates how to iterate over one batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([16, 512]),\n",
              " torch.Size([16, 512]),\n",
              " torch.Size([16, 5]),\n",
              " torch.Size([16, 5]),\n",
              " tensor([[0],\n",
              "         [0],\n",
              "         [0],\n",
              "         [0],\n",
              "         [0],\n",
              "         [0],\n",
              "         [1],\n",
              "         [0],\n",
              "         [0],\n",
              "         [0],\n",
              "         [0],\n",
              "         [1],\n",
              "         [1],\n",
              "         [0],\n",
              "         [0],\n",
              "         [1]]))"
            ]
          },
          "execution_count": 271,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model\n",
        "\n",
        "Recall that BERT only uses the encoder.\n",
        "\n",
        "BERT has the following components:\n",
        "\n",
        "- Embedding layers\n",
        "- Attention Mask\n",
        "- Encoder layer\n",
        "- Multi-head attention\n",
        "- Scaled dot product attention\n",
        "- Position-wise feed-forward network\n",
        "- BERT (assembling all the components)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Embedding\n",
        "\n",
        "<img src = \"./figures/BERT_embed.png\" width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len, n_segments, d_model, device):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
        "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        #x, seg: (bs, len)\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long).to(self.device)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.norm(embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Attention mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "id": "s1PGksqBNuZM"
      },
      "outputs": [],
      "source": [
        "def get_attn_pad_mask(seq_q, seq_k, device):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1).to(device)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the attention mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 512, 512])\n"
          ]
        }
      ],
      "source": [
        "print(get_attn_pad_mask(input_ids, input_ids, 'cpu').shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Encoder\n",
        "\n",
        "The encoder has two main components: \n",
        "\n",
        "- Multi-head Attention\n",
        "- Position-wise feed-forward network\n",
        "\n",
        "First let's make the wrapper called `EncoderLayer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, d_ff, d_k, device):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention(n_heads, d_model, d_k, device)\n",
        "        self.pos_ffn       = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
        "        return enc_outputs, attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define the scaled dot attention, to be used inside the multihead attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k, device):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([d_k])).to(device)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / self.scale # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the Multiheadattention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, d_k, device):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_k\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, self.d_v * n_heads)\n",
        "        self.device = device\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
        "\n",
        "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        context, attn = ScaledDotProductAttention(self.d_k, self.device)(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
        "        output = nn.Linear(self.n_heads * self.d_v, self.d_model, device=self.device)(context)\n",
        "        return nn.LayerNorm(self.d_model, device=self.device)(output + residual), attn # output: [batch_size x len_q x d_model]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the PoswiseFeedForwardNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
        "        return self.fc2(F.gelu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4 Putting them together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "id": "OZ0TJ84W4SZw"
      },
      "outputs": [],
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self, n_layers, n_heads, d_model, d_ff, d_k, n_segments, vocab_size, max_len, device):\n",
        "        super(BERT, self).__init__()\n",
        "        self.params = {'n_layers': n_layers, 'n_heads': n_heads, 'd_model': d_model,\n",
        "                       'd_ff': d_ff, 'd_k': d_k, 'n_segments': n_segments,\n",
        "                       'vocab_size': vocab_size, 'max_len': max_len}\n",
        "        self.embedding = Embedding(vocab_size, max_len, n_segments, d_model, device)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(n_heads, d_model, d_ff, d_k, device) for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.activ = nn.Tanh()\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        # decoder is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        n_vocab, n_dim = embed_weight.size()\n",
        "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
        "        self.decoder.weight = embed_weight\n",
        "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
        "        for layer in self.layers:\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
        "        \n",
        "        # 1. predict next sentence\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled   = self.activ(self.fc(output[:, 0])) # [batch_size, d_model]\n",
        "        logits_nsp = self.classifier(h_pooled) # [batch_size, 2]\n",
        "\n",
        "        # 2. predict the masked token\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
        "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked  = self.norm(F.gelu(self.linear(h_masked)))\n",
        "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
        "\n",
        "        return logits_lm, logits_nsp\n",
        "    \n",
        "    def get_last_hidden_state(self, input_ids, segment_ids):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
        "        for layer in self.layers:\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UAG3SEP4UbU",
        "outputId": "bc6f202f-df37-4fac-843c-fb86bdb777b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "#make our work comparable if restarted the kernel\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_layers = 6    # number of Encoder of Encoder Layer\n",
        "n_heads  = 8    # number of heads in Multi-Head Attention\n",
        "d_model  = 768  # Embedding Size\n",
        "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
        "d_k = d_v = 64  # dimension of K(=Q), V\n",
        "n_segments = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = BERT(n_layers, n_heads, d_model, d_ff, d_k, n_segments, len(vocab), max_len, device).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-5)  # BERT paper used 5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, data, optimizer, criterion, device):\n",
        "    epoch_loss = []\n",
        "    model.train()\n",
        "\n",
        "    for input_ids, segment_ids, masked_tokens, masked_pos, isNext in tqdm(data, desc='Training: '):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = input_ids.to(device)\n",
        "        segment_ids = segment_ids.to(device)\n",
        "        masked_tokens = masked_tokens.to(device)\n",
        "        masked_pos = masked_pos.to(device)\n",
        "        isNext = isNext.flatten().to(device)\n",
        "        logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)    \n",
        "        #logits_lm: (bs, max_mask, vocab_size) ==> (6, 5, 34)\n",
        "        #logits_nsp: (bs, yes/no) ==> (6, 2)\n",
        "\n",
        "        #1. mlm loss\n",
        "        #logits_lm.transpose: (bs, vocab_size, max_mask) vs. masked_tokens: (bs, max_mask)\n",
        "        loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
        "        loss_lm = (loss_lm.float()).mean()\n",
        "        #2. nsp loss\n",
        "        #logits_nsp: (bs, 2) vs. isNext: (bs, )\n",
        "        loss_nsp = criterion(logits_nsp, isNext) # for sentence classification\n",
        "        \n",
        "        #3. combine loss\n",
        "        loss = loss_lm + loss_nsp\n",
        "        epoch_loss.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return np.mean(epoch_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, data, criterion, device):\n",
        "    epoch_loss = []\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():  # set the 'requires_grad' to False to speed up computation and reduce memory cost\n",
        "        for input_ids, segment_ids, masked_tokens, masked_pos, isNext in tqdm(data, desc='Evaluate: '):\n",
        "            input_ids = input_ids.to(device)\n",
        "            segment_ids = segment_ids.to(device)\n",
        "            masked_tokens = masked_tokens.to(device)\n",
        "            masked_pos = masked_pos.to(device)\n",
        "            isNext = isNext.flatten().to(device)\n",
        "            logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)    \n",
        "            #logits_lm: (bs, max_mask, vocab_size) ==> (6, 5, 34)\n",
        "            #logits_nsp: (bs, yes/no) ==> (6, 2)\n",
        "\n",
        "            #1. mlm loss\n",
        "            #logits_lm.transpose: (bs, vocab_size, max_mask) vs. masked_tokens: (bs, max_mask)\n",
        "            loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
        "            loss_lm = (loss_lm.float()).mean()\n",
        "            #2. nsp loss\n",
        "            #logits_nsp: (bs, 2) vs. isNext: (bs, )\n",
        "            loss_nsp = criterion(logits_nsp, isNext) # for sentence classification\n",
        "            \n",
        "            #3. combine loss\n",
        "            loss = loss_lm + loss_nsp\n",
        "            epoch_loss.append(loss.item())\n",
        "\n",
        "    return np.mean(epoch_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {},
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 6/6 [02:43<00:00, 27.31s/it]\n",
            "Evaluate: 100%|██████████| 1/1 [00:07<00:00,  7.38s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 2m 51s\n",
            "\tTrain Loss: 23.529\n",
            "\t Val. Loss: 27.009\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 6/6 [02:38<00:00, 26.49s/it]\n",
            "Evaluate: 100%|██████████| 1/1 [00:07<00:00,  7.08s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 2m 46s\n",
            "\tTrain Loss: 12.805\n",
            "\t Val. Loss: 9.155\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 6/6 [02:48<00:00, 28.14s/it]\n",
            "Evaluate: 100%|██████████| 1/1 [00:07<00:00,  7.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 2m 56s\n",
            "\tTrain Loss: 8.529\n",
            "\t Val. Loss: 7.790\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 6/6 [02:27<00:00, 24.64s/it]\n",
            "Evaluate: 100%|██████████| 1/1 [00:07<00:00,  7.08s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 2m 34s\n",
            "\tTrain Loss: 10.065\n",
            "\t Val. Loss: 6.322\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 6/6 [02:31<00:00, 25.22s/it]\n",
            "Evaluate: 100%|██████████| 1/1 [00:06<00:00,  6.98s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Time: 2m 38s\n",
            "\tTrain Loss: 6.418\n",
            "\t Val. Loss: 7.046\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 6/6 [02:18<00:00, 23.02s/it]\n",
            "Evaluate: 100%|██████████| 1/1 [00:07<00:00,  7.07s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 06 | Time: 2m 25s\n",
            "\tTrain Loss: 8.742\n",
            "\t Val. Loss: 6.443\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 6/6 [02:26<00:00, 24.47s/it]\n",
            "Evaluate: 100%|██████████| 1/1 [00:07<00:00,  7.76s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 07 | Time: 2m 34s\n",
            "\tTrain Loss: 7.089\n",
            "\t Val. Loss: 5.904\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 6/6 [03:06<00:00, 31.01s/it]\n",
            "Evaluate: 100%|██████████| 1/1 [00:07<00:00,  7.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 08 | Time: 3m 13s\n",
            "\tTrain Loss: 5.906\n",
            "\t Val. Loss: 5.408\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 6/6 [02:34<00:00, 25.72s/it]\n",
            "Evaluate: 100%|██████████| 1/1 [00:06<00:00,  6.92s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 09 | Time: 2m 41s\n",
            "\tTrain Loss: 5.686\n",
            "\t Val. Loss: 4.698\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 6/6 [02:26<00:00, 24.44s/it]\n",
            "Evaluate: 100%|██████████| 1/1 [00:07<00:00,  7.77s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10 | Time: 2m 34s\n",
            "\tTrain Loss: 4.103\n",
            "\t Val. Loss: 4.110\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "save_path = './model/bert_best_model.pt'\n",
        "num_epoch = 10\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "    lr_scheduler.step(val_loss)\n",
        "\n",
        "    #for plotting\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    end_time = time.time()\n",
        "        \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # save the model only when its validation loss is lower than all its predecessors\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save([model.params, model.state_dict()], save_path)  # save the model's parameters and state to a file\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'loss')"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(5, 3))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.plot(train_losses, label = 'train loss')\n",
        "ax.plot(val_losses, label = 'validation loss')\n",
        "plt.legend()\n",
        "ax.set_xlabel('updates')\n",
        "ax.set_ylabel('loss')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inference\n",
        "\n",
        "Since our dataset is very small, it won't work very well, but just for the sake of demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load the model and all its hyperparameters\n",
        "params, state = torch.load(save_path)\n",
        "model = BERT(**params, device=device).to(device)\n",
        "model.load_state_dict(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = torch.load('./model/vocab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17825"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text1', 'text2', 'label'],\n",
              "    num_rows: 15\n",
              "})"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'and', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'and', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'and', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'and', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'and', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[MASK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[SEP]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'they', '[UNK]', 'people', '[UNK]', '[UNK]', '[UNK]', 'they', '[UNK]', 'people', '[UNK]', '[UNK]', '[MASK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'they', '[UNK]', 'people', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'they', '[UNK]', 'people', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'eyes', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'and', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'they', '[UNK]', 'people', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[MASK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'and', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'and', '[UNK]', '[UNK]', '[UNK]', '[MASK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'it', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'and', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[SEP]']\n",
            "masked tokens (words) :  ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n",
            "masked tokens list :  [4, 4, 4, 4, 4]\n",
            "predicted masked tokens (words) :  ['[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]']\n",
            "predicted masked tokens list :  [1, 1, 1, 1, 1]\n",
            "0\n",
            "isNext :  False\n",
            "predict isNext :  False\n"
          ]
        }
      ],
      "source": [
        "# Predict mask tokens ans isNext\n",
        "for input_ids, segment_ids, masked_tokens, masked_pos, isNext in val_loader:\n",
        "    break\n",
        "\n",
        "idx = 2\n",
        "input_ids = input_ids[idx].reshape(1, -1).to(device)\n",
        "segment_ids = segment_ids[idx].reshape(1, -1).to(device)\n",
        "masked_tokens = masked_tokens[idx].reshape(1, -1).to(device)\n",
        "masked_pos = masked_pos[idx].reshape(1, -1).to(device)\n",
        "isNext = isNext[idx].item()\n",
        "\n",
        "print([vocab.get_itos()[w.item()] for w in input_ids[0] if vocab.get_itos()[w.item()] != '[PAD]'])\n",
        "\n",
        "logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
        "#logits_lm:  (1, max_mask, vocab_size) ==> (1, 5, 34)\n",
        "#logits_nsp: (1, yes/no) ==> (1, 2)\n",
        "\n",
        "#predict masked tokens\n",
        "#max the probability along the vocab dim (2), [1] is the indices of the max, and [0] is the first value\n",
        "logits_lm = logits_lm.data.cpu().max(2)[1][0].data.numpy() \n",
        "#note that zero is padding we add to the masked_tokens\n",
        "print('masked tokens (words) : ',[vocab.get_itos()[pos.item()] for pos in masked_tokens[0] if pos.item() != 0])\n",
        "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
        "print('predicted masked tokens (words) : ',[vocab.get_itos()[pos.item()] for pos in logits_lm if pos.item() != 0])\n",
        "print('predicted masked tokens list : ', [pos for pos in logits_lm if pos.item() != 0])\n",
        "\n",
        "#predict nsp\n",
        "logits_nsp = logits_nsp.data.cpu().max(1)[1][0].data.numpy()\n",
        "print(logits_nsp)\n",
        "print('isNext : ', True if isNext else False)\n",
        "print('predict isNext : ', True if logits_nsp else False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text1', 'text2', 'label'],\n",
              "    num_rows: 15\n",
              "})"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BERT(\n",
              "  (embedding): Embedding(\n",
              "    (tok_embed): Embedding(93, 768)\n",
              "    (pos_embed): Embedding(512, 768)\n",
              "    (seg_embed): Embedding(2, 768)\n",
              "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (layers): ModuleList(\n",
              "    (0-5): 6 x EncoderLayer(\n",
              "      (enc_self_attn): MultiHeadAttention(\n",
              "        (W_Q): Linear(in_features=768, out_features=512, bias=True)\n",
              "        (W_K): Linear(in_features=768, out_features=512, bias=True)\n",
              "        (W_V): Linear(in_features=768, out_features=512, bias=True)\n",
              "      )\n",
              "      (pos_ffn): PoswiseFeedForwardNet(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (activ): Tanh()\n",
              "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (decoder): Linear(in_features=768, out_features=93, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 250,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Load the model, vocabulary, and parameters\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "params, state_dict = torch.load('./model/bert.pt', map_location=device)\n",
        "vocab = torch.load('./model/vocab.pth', map_location=device)\n",
        "\n",
        "# Initialize the model and load the saved state\n",
        "model = BERT(**params, device=device).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'a', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '(', '[UNK]', ')', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'a', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[MASK]', '[UNK]', '[UNK]', '[MASK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'a', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '(', '[UNK]', ')', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '(', '[UNK]', ')', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'a', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'a', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'a', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[MASK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'a', '[UNK]', '(', '[UNK]', '[UNK]', ')', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'a', '[UNK]', '[SEP]', '[UNK]', '[UNK]', '[UNK]', '[MASK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'a', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'a', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '1', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[SEP]']\n",
            "Masked tokens (words): ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n",
            "Predicted masked tokens (words): ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n",
            "isNext: False\n",
            "Predicted isNext: False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "def process_data_point(idx, input_ids, segment_ids, masked_tokens, masked_pos, isNext, device, model, vocab):\n",
        "    \"\"\"\n",
        "    Process a single data point and print the prediction results.\n",
        "    \"\"\"\n",
        "    # Reshape and send to device\n",
        "    input_ids = input_ids[idx].reshape(1, -1).to(device)\n",
        "    segment_ids = segment_ids[idx].reshape(1, -1).to(device)\n",
        "    masked_tokens = masked_tokens[idx].reshape(1, -1).to(device)\n",
        "    masked_pos = masked_pos[idx].reshape(1, -1).to(device)\n",
        "    isNext = isNext[idx].item()\n",
        "\n",
        "    print([vocab.get_itos()[w.item()] for w in input_ids[0] if vocab.get_itos()[w.item()] != '[PAD]'])\n",
        "\n",
        "    logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
        "\n",
        "    # Process language model logits\n",
        "    logits_lm = logits_lm.data.cpu().max(2)[1][0].data.numpy()\n",
        "    print('Masked tokens (words):', [vocab.get_itos()[pos.item()] for pos in masked_tokens[0] if pos.item() != 0])\n",
        "    print('Predicted masked tokens (words):', [vocab.get_itos()[pos.item()] for pos in logits_lm if pos.item() != 0])\n",
        "\n",
        "    # Process next sentence prediction logits\n",
        "    logits_nsp = logits_nsp.data.cpu().max(1)[1][0].data.numpy()\n",
        "    print('isNext:', True if isNext else False)\n",
        "    print('Predicted isNext:', True if logits_nsp else False)\n",
        "\n",
        "# Predict mask tokens and isNext\n",
        "try:\n",
        "    for input_ids, segment_ids, masked_tokens, masked_pos, isNext in val_loader:\n",
        "        # Randomly select an index from the batch\n",
        "        batch_size = input_ids.size(0)\n",
        "        idx = random.randint(0, batch_size - 1)\n",
        "\n",
        "        process_data_point(idx, input_ids, segment_ids, masked_tokens, masked_pos, isNext, device, model, vocab)\n",
        "        break  # Only process one batch for demonstration\n",
        "except Exception as e:\n",
        "    print(f\"Error processing batch: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text 1:\n",
            " fsfsdfsfssfs econometric studies have shown that this effect cannot be explained by a variety of alternative factors including differential trends across areas changing crop prices shifts in certain educational and health policies and the effect of malaria eradication no significant contemporaneous results were found for adults who should have benefited less from the intervention owing to their substantially lower (prior) infection rates the program nearly eradicated hookworm and would flourish afterward with new funding as the rockefeller foundation international health divisionthe rfs hookworm campaign in mexico showed how science and politics play a role in developing health policies it brought together government officials health officials public health workers rockefeller officials and the community this campaign was launched to eradicate hookworms in mexico although the campaign did not focus on longterm treatments it did set the terms of the relationship between mexico and the rockefeller foundation the scientific knowledge behind this campaign helped shape public health policies improved public health and built a strong relationship between us and mexicoin the 1920s hookworm eradication reached the caribbean and latin america where great mortality was reported among people in the west indies towards the end of the 18th century as well as through descriptions sent from brazil and various other tropical and subtropical regions treatments\n",
            "treatment in the early 20th century relied on the use of epsom salt to reduce protective mucus followed by thymol to kill the worms by the 1940s tetrachloroethylene was the leading method\n",
            "\n",
            "Original Text 2:\n",
            " \"spinraza access by country\" treatsma 18 october 2018 retrieved 20190528\n",
            "\n",
            "Are sentences next? No (Original)\n",
            "\n",
            "Predicted NSP: No\n",
            "\n",
            "NSP Score: 0.6636 (Confidence)\n"
          ]
        }
      ],
      "source": [
        "# The specific input example you provided\n",
        "input_example = {\n",
        "    'text1': 'fsfsdfsfssfs econometric studies have shown that this effect cannot be explained by a variety of alternative factors including differential trends across areas changing crop prices shifts in certain educational and health policies and the effect of malaria eradication no significant contemporaneous results were found for adults who should have benefited less from the intervention owing to their substantially lower (prior) infection rates the program nearly eradicated hookworm and would flourish afterward with new funding as the rockefeller foundation international health divisionthe rfs hookworm campaign in mexico showed how science and politics play a role in developing health policies it brought together government officials health officials public health workers rockefeller officials and the community this campaign was launched to eradicate hookworms in mexico although the campaign did not focus on longterm treatments it did set the terms of the relationship between mexico and the rockefeller foundation the scientific knowledge behind this campaign helped shape public health policies improved public health and built a strong relationship between us and mexicoin the 1920s hookworm eradication reached the caribbean and latin america where great mortality was reported among people in the west indies towards the end of the 18th century as well as through descriptions sent from brazil and various other tropical and subtropical regions treatments\\ntreatment in the early 20th century relied on the use of epsom salt to reduce protective mucus followed by thymol to kill the worms by the 1940s tetrachloroethylene was the leading method',\n",
        "    'text2': '\"spinraza access by country\" treatsma 18 october 2018 retrieved 20190528',\n",
        "    'isNext': False\n",
        "}\n",
        "\n",
        "# Assuming 'tokenizer', 'vocab', and 'device' are already defined and loaded\n",
        "\n",
        "# Tokenize the input texts\n",
        "text1_tokens = tokenizer(input_example['text1'])\n",
        "text2_tokens = tokenizer(input_example['text2'])\n",
        "\n",
        "\n",
        "# Convert tokens to vocab indices\n",
        "tokens_a_idx = [vocab[token] if token in vocab else vocab['[UNK]'] for token in text1_tokens]\n",
        "tokens_b_idx = [vocab[token] if token in vocab else vocab['[UNK]'] for token in text2_tokens]\n",
        "\n",
        "# Prepare model inputs\n",
        "input_ids = [vocab['[CLS]']] + tokens_a_idx + [vocab['[SEP]']] + tokens_b_idx + [vocab['[SEP]']]\n",
        "segment_ids = [0] * (len(tokens_a_idx) + 2) + [1] * (len(tokens_b_idx) + 1)\n",
        "input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
        "segment_ids = torch.tensor(segment_ids).unsqueeze(0).to(device)\n",
        "masked_pos = torch.tensor([]).unsqueeze(0).to(device)  # No masked tokens in the inference example\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
        "\n",
        "# Predicted isNext flag and confidence score\n",
        "predicted_isNext = logits_nsp.argmax(dim=-1).item()\n",
        "confidence_score = torch.nn.functional.softmax(logits_nsp, dim=-1).max().item()\n",
        "predicted_isNext_text = \"Yes\" if predicted_isNext else \"No\"\n",
        "original_isNext = \"Yes\" if input_example['isNext'] else \"No\"\n",
        "\n",
        "# Display results\n",
        "print(\"Original Text 1:\\n\", input_example['text1'])\n",
        "print(\"\\nOriginal Text 2:\\n\", input_example['text2'])\n",
        "print(f\"\\nAre sentences next? {original_isNext} (Original)\")\n",
        "print(f\"\\nPredicted NSP: {predicted_isNext_text}\")\n",
        "print(f\"\\nNSP Score: {confidence_score:.4f} (Confidence)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text 1: lincomycin is a lincosamide antibiotic that comes from the actinomycete streptomyces lincolnensis a related compound clindamycin is derived from lincomycin by using thionyl chloride to replace the 7hydroxy group with a chlorine atom with inversion of chirality it was released for medical use in september 1964 uses\n",
            "although similar in antibacterial spectrum and mechanism of action to macrolides lincomycin is also effective against other organisms including actinomycetes and some species of mycoplasma and plasmodiumhowever because of its adverse effects and toxicity it is rarely used today and reserved for patients allergic to penicillin or where bacteria have developed resistance clinical pharmacology\n",
            "intramuscular administration of a single dose of 600 mg of lincomycin produces average peak serum levels of 116 µg/ml at 60 min and maintains therapeutic levels for 17 h to 20 h for most susceptible grampositive organisms urinary excretion after this dose ranges from 18% to 248% (mean: 173%) a twohour intravenous infusion of 600 mg of lincomycin achieves average peak serum levels of 159 µg/ml and yields therapeutic levels for 14 h for most susceptible grampositive organisms urinary excretion ranges from 49% to 303% (mean: 138%) the biological halflife after im or iv administration is 54 ± 10 h  the serum halflife of lincomycin may be prolonged in patients with severe impairment of renal function compared to patients with normal renal function in patients with abnormal hepatic function serum halflife may be twofold longer than in patients with normal hepatic function hemodialysis and peritoneal dialysis are not effective in removing lincomycin from the serum\n",
            "Original Text 2: society and culture\n",
            "legal status\n",
            "on 27 january 2022 the committee for medicinal products for human use (chmp) of the european medicines agency (ema) adopted a positive opinion recommending the granting of a marketing authorization for the medicinal product breyanzi intended for the treatment of adults with relapsed or refractory diffuse large b cell lymphoma (dlbcl) primary mediastinal large bcell lymphoma (pmbcl) and follicular lymphoma grade 3b (fl3b) after at least two previous lines of treatments the applicant for this medicinal product is bristolmyers squibb pharma eeig lisocabtagene maraleucel was approved for medical use in the european union in april 2022 names\n",
            "lisocabtagene maraleucel is the international nonproprietary name (inn) references\n",
            "external links\n",
            "\"lisocabtagene maraleucel\" nci drug dictionary national cancer institute clinical trial number nct02631044 for \"study evaluating the safety and pharmacokinetics of jcar017 in bcell nonhodgkin lymphoma (transcendnhl001)\" at clinicaltrialsgov\n",
            "manali kamdar md mbbs an associate professor at the university of colorado denver gives a video interview entitled lisocabtagene maraleucel fda approved for large bcell lymphoma  https://oncologytubecom/video/41258\n",
            "Masked Text 1: [MASK] is a lincosamide antibiotic that comes [MASK] the actinomycete streptomyces lincolnensis a related [MASK] clindamycin is derived from lincomycin by [MASK] thionyl chloride to replace the 7hydroxy group with a chlorine atom with inversion of [MASK] it was released for medical use in september 1964 [MASK] although similar in antibacterial spectrum and mechanism of action to macrolides lincomycin [MASK] also effective [MASK] [MASK] organisms including actinomycetes and some species of [MASK] and plasmodiumhowever because of its adverse effects and toxicity it is rarely used today and reserved for patients allergic to penicillin or where bacteria have developed [MASK] clinical [MASK] intramuscular administration of a single dose of 600 [MASK] [MASK] lincomycin produces average peak serum levels of 116 µg/ml at 60 [MASK] and [MASK] therapeutic levels for 17 h to 20 h for most susceptible grampositive organisms urinary excretion [MASK] this dose [MASK] from 18% to [MASK] [MASK] mean 173% ) a twohour intravenous infusion of 600 mg of lincomycin achieves average [MASK] [MASK] levels of 159 µg/ml and yields therapeutic levels for 14 h for [MASK] susceptible grampositive organisms urinary excretion ranges from 49% to 303% ( mean 138% [MASK] the biological halflife [MASK] im or iv administration is 54 ± 10 h the serum halflife of lincomycin may [MASK] prolonged in patients with severe impairment of renal function compared to patients [MASK] normal renal function in patients [MASK] abnormal [MASK] function serum halflife may be [MASK] longer than in [MASK] with normal hepatic function hemodialysis [MASK] peritoneal dialysis are not effective in removing lincomycin from the serum\n",
            "Masked Text 2: [MASK] [MASK] [MASK] legal status [MASK] [MASK] [MASK] 2022 [MASK] committee for medicinal products for human [MASK] ( chmp ) of the european medicines agency ( ema ) adopted a positive opinion [MASK] the granting of a marketing [MASK] for the [MASK] product [MASK] intended [MASK] the treatment of [MASK] with relapsed or refractory diffuse [MASK] b cell lymphoma ( [MASK] ) primary mediastinal large bcell lymphoma ( [MASK] ) and follicular [MASK] grade 3b [MASK] fl3b ) after at least two previous lines of treatments the applicant for this medicinal product is [MASK] squibb pharma eeig lisocabtagene maraleucel was approved [MASK] medical use in the european union [MASK] april 2022 [MASK] [MASK] maraleucel is the international [MASK] name ( inn [MASK] references external links lisocabtagene [MASK] nci drug dictionary national cancer institute clinical [MASK] number nct02631044 for study evaluating the safety and pharmacokinetics of jcar017 in [MASK] nonhodgkin lymphoma [MASK] transcendnhl001 ) at clinicaltrialsgov manali kamdar md mbbs [MASK] associate [MASK] [MASK] [MASK] university of colorado denver gives a video interview entitled lisocabtagene maraleucel fda approved [MASK] large [MASK] lymphoma https //oncologytubecom/video/41258\n",
            "Are sentences next? No (Original)\n",
            "Predicted NSP: No\n",
            "NSP Score: 0.6607 (Confidence)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "# Assuming 'model', 'tokenizer', 'vocab', 'device' are correctly defined\n",
        "\n",
        "# Function to mask tokens\n",
        "def mask_tokens(tokens, vocab, probability=0.15):\n",
        "    masked_tokens = []\n",
        "    masked_positions = []\n",
        "    for i, token in enumerate(tokens):\n",
        "        if random.random() < probability:\n",
        "            masked_tokens.append('[MASK]')\n",
        "            masked_positions.append(i)\n",
        "        else:\n",
        "            masked_tokens.append(token)\n",
        "    return masked_tokens, masked_positions\n",
        "\n",
        "# Randomly select an example from the test dataset\n",
        "random_idx = random.randint(0, len(transformed_dataset['test']) - 1)\n",
        "input_example = transformed_dataset['test'][random_idx]\n",
        "\n",
        "# Tokenize the texts\n",
        "text1_tokens = tokenizer(input_example['text1'])\n",
        "text2_tokens = tokenizer(input_example['text2'])\n",
        "\n",
        "# Mask tokens\n",
        "masked_text1_tokens, _ = mask_tokens(text1_tokens, vocab)\n",
        "masked_text2_tokens, _ = mask_tokens(text2_tokens, vocab)\n",
        "\n",
        "# Convert tokens to indices, handling unknown tokens\n",
        "tokens_a_idx = [vocab[token] if token in vocab else vocab['[UNK]'] for token in masked_text1_tokens]\n",
        "tokens_b_idx = [vocab[token] if token in vocab else vocab['[UNK]'] for token in masked_text2_tokens]\n",
        "\n",
        "# Prepare inputs\n",
        "input_ids = [vocab['[CLS]']] + tokens_a_idx + [vocab['[SEP]']] + tokens_b_idx + [vocab['[SEP]']]\n",
        "segment_ids = [0] * (len(tokens_a_idx) + 2) + [1] * (len(tokens_b_idx) + 1)\n",
        "input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
        "segment_ids = torch.tensor(segment_ids).unsqueeze(0).to(device)\n",
        "\n",
        "# Inference\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    _, logits_nsp = model(input_ids, segment_ids, torch.tensor([]).unsqueeze(0).to(device))\n",
        "\n",
        "# Display results\n",
        "print(\"Original Text 1:\", input_example['text1'])\n",
        "print(\"Original Text 2:\", input_example['text2'])\n",
        "print(\"Masked Text 1:\", ' '.join(masked_text1_tokens))\n",
        "print(\"Masked Text 2:\", ' '.join(masked_text2_tokens))\n",
        "predicted_isNext = logits_nsp.argmax(dim=-1).item()\n",
        "confidence_score = torch.nn.functional.softmax(logits_nsp, dim=-1).max().item()\n",
        "predicted_isNext_text = \"Yes\" if predicted_isNext else \"No\"\n",
        "original_isNext_text = \"Yes\" if input_example['isNext'] else \"No\"\n",
        "print(f\"Are sentences next? {original_isNext_text} (Original)\")\n",
        "print(f\"Predicted NSP: {predicted_isNext_text}\")\n",
        "print(f\"NSP Score: {confidence_score:.4f} (Confidence)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text 1:\n",
            " these and other novel genes (dpti dptj) are believed to be involved in supplying the nonproteinogenic amino acids l3methylglutamic acid and kyn; they are located next to the nrps genesthe decanoic acid portion of daptomycin is synthesized by fatty acid synthase machinery (figure 2) posttranslational modification of the apoacyl carrier protein (acp thiolation or t domain) by a phosphopantetheinyltransferase (pptase) enzyme catalyzes the transfer of a flexible phosphopantetheine arm from coenzyme a to a conserved serine in the acp domain through a phosphodiester linkage the holoacp can provide a thiol on which the substrate and acyl chains are covalently bound during chain elongations the two core catalytic domains are an acyltransferase (at) and a ketosynthase (ks) the at acts upon a malonylcoa substrate and transfers an acyl group to the thiol of the acp domain this net transthiolation is an energyneutral step next the acylsacp gets transthiolated to a conserved cysteine on the ks; the ks decarboxylates the downstream malonylsacp and forms a βketoacylsacp this serves as the substrate for the next cycle of elongation before the next cycle begins however the βketo group undergoes reduction to the corresponding alcohol catalyzed by a ketoreductase domain followed by dehydration to the olefin catalyzed by a dehydratase domain and finally reduction to the methylene catalyzed by an enoylreductase domain each ks catalytic cycle results in the net addition of two carbons\n",
            "\n",
            "Original Text 2:\n",
            " the domain organization in such modules is catethe first module has a threedomain cat organization; these often occur in assembly lines that make nacylated peptides the first c domain catalyzes nacylation of the initiating amino acid (tryptophan) while it is installed on t  an adenylating enzyme (ad) catalyzes the condensation of decanoic acid and the nterminal tryptophan which incorporates decanoic acid into the growing peptide (figure 3) the genes responsible for this coupling event are dpte and dptf which are located upstream of dpta the first gene of the daptomycin nrps biosynthetic gene cluster once the coupling of decanoic acid to the nterminal tryptophan residue occurs the condensation of amino acids begins catalyzed by the nrpsthe first five modules of the nrps are encoded by the dpta gene and catalyze the condensation of ltryptophan dasparagine laspartate lthreonine and glycine respectively (figure 4) modules 6–11 which catalyze the condensation of lornithine laspartate dalanine laspartate glycine and dserine are encoded for the dptbc gene (figure 5) dptd catalyzes the incorporation of two nonproteinogenic amino acids l3methylglutamic acid (mglu) and kyn which is only known thus far to daptomycin into the growing peptide (figure 6) elongation by these nrps modules ultimately leads to macrocyclization and release in which an αamino group namely threonine acts as an internal nucleophile during cyclization to yield the 10aminoacid ring (figure 6) the termination module in the nrps assembly line has a catte organization\n",
            "\n",
            "Are sentences next? Yes (Original)\n",
            "\n",
            "Predicted NSP: No\n",
            "\n",
            "NSP Score: 0.6538 (Confidence)\n"
          ]
        }
      ],
      "source": [
        "# Select a random sample from transformed_dataset['test']\n",
        "random_idx = randint(0, len(transformed_dataset['test']) - 1)\n",
        "input_example = transformed_dataset['test'][random_idx]\n",
        "\n",
        "\n",
        "# Assuming 'tokenizer', 'vocab', and 'device' are already defined and loaded\n",
        "\n",
        "# Tokenize the input texts\n",
        "text1_tokens = tokenizer(input_example['text1'])\n",
        "text2_tokens = tokenizer(input_example['text2'])\n",
        "\n",
        "\n",
        "# Convert tokens to vocab indices\n",
        "tokens_a_idx = [vocab[token] if token in vocab else vocab['[UNK]'] for token in text1_tokens]\n",
        "tokens_b_idx = [vocab[token] if token in vocab else vocab['[UNK]'] for token in text2_tokens]\n",
        "\n",
        "# Prepare model inputs\n",
        "input_ids = [vocab['[CLS]']] + tokens_a_idx + [vocab['[SEP]']] + tokens_b_idx + [vocab['[SEP]']]\n",
        "segment_ids = [0] * (len(tokens_a_idx) + 2) + [1] * (len(tokens_b_idx) + 1)\n",
        "input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
        "segment_ids = torch.tensor(segment_ids).unsqueeze(0).to(device)\n",
        "masked_pos = torch.tensor([]).unsqueeze(0).to(device)  # No masked tokens in the inference example\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
        "\n",
        "# Predicted isNext flag and confidence score\n",
        "predicted_isNext = logits_nsp.argmax(dim=-1).item()\n",
        "confidence_score = torch.nn.functional.softmax(logits_nsp, dim=-1).max().item()\n",
        "predicted_isNext_text = \"Yes\" if predicted_isNext else \"No\"\n",
        "original_isNext = \"Yes\" if input_example['isNext'] else \"No\"\n",
        "\n",
        "# Display results\n",
        "print(\"Original Text 1:\\n\", input_example['text1'])\n",
        "print(\"\\nOriginal Text 2:\\n\", input_example['text2'])\n",
        "print(f\"\\nAre sentences next? {original_isNext} (Original)\")\n",
        "print(f\"\\nPredicted NSP: {predicted_isNext_text}\")\n",
        "print(f\"\\nNSP Score: {confidence_score:.4f} (Confidence)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BERT(\n",
              "  (embedding): Embedding(\n",
              "    (tok_embed): Embedding(93, 768)\n",
              "    (pos_embed): Embedding(512, 768)\n",
              "    (seg_embed): Embedding(2, 768)\n",
              "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (layers): ModuleList(\n",
              "    (0-5): 6 x EncoderLayer(\n",
              "      (enc_self_attn): MultiHeadAttention(\n",
              "        (W_Q): Linear(in_features=768, out_features=512, bias=True)\n",
              "        (W_K): Linear(in_features=768, out_features=512, bias=True)\n",
              "        (W_V): Linear(in_features=768, out_features=512, bias=True)\n",
              "      )\n",
              "      (pos_ffn): PoswiseFeedForwardNet(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (activ): Tanh()\n",
              "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (decoder): Linear(in_features=768, out_features=93, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 287,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
