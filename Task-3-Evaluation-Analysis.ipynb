{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imported Libraries\n",
    "\n",
    "The code imports essential Python libraries for mathematical operations (`math`), regular expressions (`re`), random number generation (`random`), numerical computing (`numpy`), and deep learning with PyTorch (`torch`). Specifically, PyTorch modules for neural networks (`torch.nn`) and optimization (`torch.optim`) are imported. These imports lay the groundwork for various computational tasks, including data manipulation, model building, and optimization.\n",
    "\n",
    "The code initializes various libraries and checks whether CUDA is available to use GPU acceleration. If CUDA is available, it sets the device to CUDA; otherwise, it uses the CPU. This setup ensures that the code can run efficiently on the available hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "from   random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data for Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment imports datasets and loads two specific datasets, SNLI and MNLI, using the Hugging Face `datasets` library. The SNLI dataset is loaded directly using `load_dataset` method with the dataset name 'snli', while the MNLI dataset is loaded using the same method with 'glue' as the dataset name and 'mnli' as the task name. \n",
    "\n",
    "After loading the datasets, the code retrieves and prints the features of the training splits of both MNLI and SNLI datasets using the attributes `features`. This provides insight into the structure and available features within the datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shafisourov/anaconda3/envs/nlu/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'premise': Value(dtype='string', id=None),\n",
       "  'hypothesis': Value(dtype='string', id=None),\n",
       "  'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None),\n",
       "  'idx': Value(dtype='int32', id=None)},\n",
       " {'premise': Value(dtype='string', id=None),\n",
       "  'hypothesis': Value(dtype='string', id=None),\n",
       "  'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None)})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "snli = datasets.load_dataset('snli')\n",
    "mnli = datasets.load_dataset('glue', 'mnli')\n",
    "mnli['train'].features, snli['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of datasets to remove 'idx' column from\n",
    "mnli.column_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'idx' column from each dataset\n",
    "for column_names in mnli.column_names.keys():\n",
    "    mnli[column_names] = mnli[column_names].remove_columns('idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli.column_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([-1,  0,  1,  2]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(mnli['train']['label']), np.unique(snli['train']['label'])\n",
    "#snli also have -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are -1 values in the label feature, these are where no class could be decided so we remove\n",
    "snli = snli.filter(\n",
    "    lambda x: 0 if x['label'] == -1 else 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli = mnli.filter(\n",
    "    lambda x: 0 if x['label'] == -1 else 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([0, 1, 2]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(mnli['train']['label']), np.unique(snli['train']['label'])\n",
    "#snli also have -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code segment, two `DatasetDict` objects, `snli` and `mnli`, are merged into a single `DatasetDict` object named `raw_dataset`. The `DatasetDict` objects contain datasets for training, testing, and validation splits.\n",
    "\n",
    "- The training split of `snli` and `mnli` datasets are concatenated and shuffled with a seed value of 55. The first 85 samples from the concatenated dataset are selected to form the training split of the merged dataset.\n",
    "- The testing split of `snli` and `mnli` datasets are concatenated and shuffled with a seed value of 55. The first 15 samples from the concatenated dataset are selected to form the testing split of the merged dataset.\n",
    "- The validation split of `snli` and `mnli` datasets are concatenated and shuffled with a seed value of 55. The first 15 samples from the concatenated dataset are selected to form the validation split of the merged dataset.\n",
    "\n",
    "The resulting `raw_dataset` contains the combined datasets from `snli` and `mnli` with the specified splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 85\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 15\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 15\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you have your two DatasetDict objects named snli and mnli\n",
    "from datasets import DatasetDict\n",
    "# Merge the two DatasetDict objects\n",
    "raw_dataset = DatasetDict({\n",
    "    'train': datasets.concatenate_datasets([snli['train'], mnli['train']]).shuffle(seed=55).select(list(range(85))),\n",
    "    'test': datasets.concatenate_datasets([snli['test'], mnli['test_mismatched']]).shuffle(seed=55).select(list(range(15))),\n",
    "    'validation': datasets.concatenate_datasets([snli['validation'], mnli['validation_mismatched']]).shuffle(seed=55).select(list(range(15)))\n",
    "})\n",
    "# Now, merged_dataset_dict contains the combined datasets from snli and mnli\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment imports the `get_tokenizer` function from `torchtext.data.utils` and loads the 'basic_english' tokenizer using this function. It then loads a vocabulary stored in a file named 'vocab.pth' located in the './model/' directory using `torch.load()` function.\n",
    "\n",
    "- The `get_tokenizer` function is used to retrieve the tokenizer for basic English text, which is commonly used for simple English language processing tasks.\n",
    "- The `torch.load()` function is utilized to load a vocabulary stored in a PyTorch binary file ('vocab.pth') from the './model/' directory. This vocabulary is presumably pre-trained or generated elsewhere for use in natural language processing tasks.\n",
    "\n",
    "The resulting `tokenizer` and `vocab` variables are now available for use in further processing or model building tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# Load the 'basic_english' tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = torch.load('./model/vocab.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of '[PAD]': 0\n",
      "Index of '[CLS]': 1\n",
      "Index of '[SEP]': 2\n",
      "Index of '[MASK]': 3\n",
      "Index of '[UNK]': 4\n",
      "Index of 'the': 4\n",
      "Index of 'of': 4\n",
      "Index of 'and': 4\n"
     ]
    }
   ],
   "source": [
    "tokens_to_check = ['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]', 'the', 'of', 'and']\n",
    "for token in tokens_to_check:\n",
    "    print(f\"Index of '{token}': {vocab[token]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world how are you doing today let's explore  regex\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sent = \"Hello, world! How are you doing today? Let's explore - regex.\"\n",
    "cleaned_sent = re.sub(\"[.,!?\\\\-]\", '', sent.lower())\n",
    "\n",
    "print(cleaned_sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment defines a series of functions to tokenize, pad, and preprocess text data for a natural language processing task. It then applies these functions to a dataset for further processing.\n",
    "\n",
    "### Functions Defined:\n",
    "1. **`tokenize_and_pad(sentences, tokenizer, vocab, max_length=512)`**:\n",
    "   - Tokenizes sentences using the provided tokenizer, converts tokens to IDs using the given vocabulary, adds special tokens (e.g., [CLS], [SEP]), and applies padding to ensure uniform sequence length.\n",
    "   - Parameters:\n",
    "     - `sentences`: List of input sentences to be tokenized and padded.\n",
    "     - `tokenizer`: Tokenizer object used to tokenize the sentences.\n",
    "     - `vocab`: Vocabulary containing token-to-ID mappings.\n",
    "     - `max_length`: Maximum sequence length after padding (default is 512).\n",
    "   - Returns:\n",
    "     - `input_ids`: List of input token IDs after tokenization and padding.\n",
    "     - `attn_mask`: List of attention masks indicating which tokens are real and which are padding tokens.\n",
    "\n",
    "2. **`preprocess_function(examples)`**:\n",
    "   - Tokenizes and pads both premise and hypothesis sentences in examples, extracting labels from the dataset.\n",
    "   - Parameters:\n",
    "     - `examples`: Dictionary containing keys for 'premise', 'hypothesis', and 'label'.\n",
    "   - Returns:\n",
    "     - Dictionary containing preprocessed data with keys:\n",
    "       - \"premise_input_ids\": Tokenized and padded input IDs for premise sentences.\n",
    "       - \"premise_attention_mask\": Attention masks for premise sentences.\n",
    "       - \"hypothesis_input_ids\": Tokenized and padded input IDs for hypothesis sentences.\n",
    "       - \"hypothesis_attention_mask\": Attention masks for hypothesis sentences.\n",
    "       - \"labels\": Extracted labels from the dataset.\n",
    "\n",
    "### Data Processing:\n",
    "- The `preprocess_function` is mapped across the `raw_dataset` using `map()` function from the `datasets` library in a batched manner.\n",
    "- The original columns ('premise', 'hypothesis', 'label') are removed to focus on the processed ones.\n",
    "- The format of the tokenized dataset is set to PyTorch tensors using `set_format(\"torch\")`.\n",
    "\n",
    "The resulting `tokenized_datasets` contains the preprocessed data ready for consumption in PyTorch-based models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "\n",
    "# Example usage before your model.forward() call\n",
    "\n",
    "\n",
    "def tokenize_and_pad(sentences, tokenizer, vocab, max_length=512):\n",
    "    # Tokenizes sentences, converts tokens to IDs, adds special tokens, and applies padding\n",
    "    tokenized = [tokenizer(re.sub(\"[.,!?\\\\-]\", '', sent.lower())) for sent in sentences]\n",
    "    input_ids = [[vocab['[CLS]']] + [vocab[token] for token in tokens] + [vocab['[SEP]']] for tokens in tokenized]\n",
    "\n",
    "    attn_mask = [[1] * len(tokens) + [0] * (max_length - len(tokens)) for tokens in input_ids]\n",
    "    input_ids = [tokens + [0] * (max_length - len(tokens)) for tokens in input_ids]\n",
    "    return input_ids, attn_mask\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize and pad both premise and hypothesis\n",
    "    premise_input_ids, premise_attn_mask = tokenize_and_pad(examples['premise'], tokenizer, vocab, max_seq_length)\n",
    "    hypothesis_input_ids, hypothesis_attn_mask = tokenize_and_pad(examples['hypothesis'], tokenizer, vocab, max_seq_length)\n",
    "    \n",
    "    # Extract labels\n",
    "    labels = examples[\"label\"]\n",
    "    \n",
    "    return {\n",
    "        \"premise_input_ids\": premise_input_ids,\n",
    "        \"premise_attention_mask\": premise_attn_mask,\n",
    "        \"hypothesis_input_ids\": hypothesis_input_ids,\n",
    "        \"hypothesis_attention_mask\": hypothesis_attn_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# Map the preprocessing function across the dataset in a batched manner\n",
    "tokenized_datasets = raw_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Remove the original columns to focus on the processed ones and set the format to PyTorch tensors\n",
    "# tokenized_datasets = tokenized_datasets.remove_columns(['premise', 'hypothesis', 'label'])\n",
    "tokenized_datasets.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment initializes three PyTorch `DataLoader` objects for training, evaluation, and testing purposes using the preprocessed tokenized datasets.\n",
    "\n",
    "- **Training DataLoader (`train_dataloader`)**:\n",
    "  - Loads data from the 'train' split of the tokenized dataset.\n",
    "  - `batch_size`: 5 (specified in the code).\n",
    "  - Shuffles the data during training (`shuffle=True`).\n",
    "\n",
    "- **Evaluation DataLoader (`eval_dataloader`)**:\n",
    "  - Loads data from the 'validation' split of the tokenized dataset.\n",
    "  - `batch_size`: 5 (specified in the code).\n",
    "  - Does not shuffle the data during evaluation (default behavior).\n",
    "\n",
    "- **Testing DataLoader (`test_dataloader`)**:\n",
    "  - Loads data from the 'test' split of the tokenized dataset.\n",
    "  - `batch_size`: 5 (specified in the code).\n",
    "  - Does not shuffle the data during testing (default behavior).\n",
    "\n",
    "### Key Parameters:\n",
    "- `batch_size`: Specifies the number of samples in each batch.\n",
    "- `shuffle`: Controls whether to shuffle the data (applicable for training only).\n",
    "- `tokenized_datasets['train']`, `tokenized_datasets['validation']`, `tokenized_datasets['test']`: Accesses the respective splits of the tokenized dataset.\n",
    "\n",
    "These `DataLoader` objects enable efficient loading of batches of data for training, evaluation, and testing of PyTorch models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# initialize the dataloader\n",
    "batch_size = 5\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets['validation'], \n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets['test'], \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch['premise_input_ids'].shape)\n",
    "    print(batch['premise_attention_mask'].shape)\n",
    "    print(batch['hypothesis_input_ids'].shape)\n",
    "    print(batch['hypothesis_attention_mask'].shape)\n",
    "    print(batch['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This code segment loads a pre-trained BERT-based model from a specified path along with its hyperparameters.\n",
    "\n",
    "- **Model Loading**:\n",
    "  - The code imports a custom model class named `BERT` from a file named `model_class.py`.\n",
    "  - It loads the model's hyperparameters and state dictionary from the specified path './model/bert_best_model.pt'.\n",
    "  - The hyperparameters and state dictionary are used to reconstruct the BERT model instance.\n",
    "  - The model is then moved to the specified device (assumed to be previously defined).\n",
    "\n",
    "### Key Components:\n",
    "- **`load_path`**: Path to the saved model state and hyperparameters.\n",
    "- **`params`**: Loaded hyperparameters of the model.\n",
    "- **`state`**: Loaded state dictionary of the model.\n",
    "- **`model`**: Instance of the `BERT` model class initialized with the loaded hyperparameters and state dictionary.\n",
    "- **`device`**: Device to move the model to.\n",
    "\n",
    "This segment effectively loads a pre-trained BERT model along with its parameters, allowing further usage for inference or fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT Scratch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # start from a pretrained bert-base-uncased model\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# model.to(device)\n",
    "from model_class import *\n",
    "\n",
    "# load the model and all its hyperparameters\n",
    "load_path = './model/bert_best_model.pt'\n",
    "params, state = torch.load(load_path)\n",
    "model_scratch = BERT(**params, device=device).to(device)\n",
    "model_scratch.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sentence BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model_class import *\n",
    "load_path = './model/best_s_bert.pt'\n",
    "params, state = torch.load(load_path)\n",
    "model_S_Bert = BERT(**params, device=device).to(device)\n",
    "model_S_Bert.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Sentence BERT Model (all-MiniLM-L12-v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shafisourov/anaconda3/envs/nlu/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Assuming you have a DataLoader named `data_loader` that provides batches of {'premise': [...], 'hypothesis': [...]} pairs\n",
    "model_pretrained_S_BERT = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parameter Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Trainable Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bert_scratch</td>\n",
       "      <td>37073759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S_Bert</td>\n",
       "      <td>37073759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pretrained_S_BERT</td>\n",
       "      <td>33360000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model  Trainable Parameters\n",
       "0       Bert_scratch              37073759\n",
       "1             S_Bert              37073759\n",
       "2  Pretrained_S_BERT              33360000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate the number of trainable parameters for each model\n",
    "num_params_Bert_scratch = sum(p.numel() for p in model_scratch.parameters() if p.requires_grad)\n",
    "num_params_S_Bert = sum(p.numel() for p in model_S_Bert.parameters() if p.requires_grad)\n",
    "num_params_pretrained_S_BERT = sum(p.numel() for p in model_pretrained_S_BERT.parameters() if p.requires_grad)\n",
    "\n",
    "# Create a DataFrame to display the results in tabular format\n",
    "data = {\n",
    "    'Model': ['Bert_scratch', 'S_Bert', 'Pretrained_S_BERT'],\n",
    "    'Trainable Parameters': [num_params_Bert_scratch, num_params_S_Bert, num_params_pretrained_S_BERT]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Below is a tabular summary of the trainable parameters for each model evaluated in the Sentence Similarity Project:\n",
    "\n",
    "| Model                | Trainable Parameters |\n",
    "|----------------------|----------------------|\n",
    "| Bert_scratch         | 37,073,759           |\n",
    "| S_Bert               | 37,073,759           |\n",
    "| Pretrained_S_BERT    | 33,360,000           |\n",
    "\n",
    "- **Bert_scratch** and **S_Bert** share an identical count of trainable parameters, indicating similar complexity in their architectures.\n",
    "- **Pretrained_S_BERT** demonstrates a streamlined architecture with fewer parameters, potentially enhancing efficiency while maintaining performance in sentence similarity tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison of Training, Validation & Test Loss, Precision, Recall & F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Scratch Model Performance on WikiMedical Sentence Similarity Dataset\n",
    "\n",
    "- Training , Validation & Test Loss\n",
    "\n",
    "| Metric              | Value   |\n",
    "|---------------------|---------|\n",
    "| Average Training Loss | 9.2872  |\n",
    "| Average Validation Loss | 8.3885  |\n",
    "| Average Test Loss   | 0.7880   |\n",
    "\n",
    "- Precision, Recall & F1-Score\n",
    "\n",
    "| Metric    | Value   |\n",
    "|-----------|---------|\n",
    "| Precision | 0.2178  |\n",
    "| Recall    | 0.4667  |\n",
    "| F1 Score  | 0.2970  |\n",
    "\n",
    "### Device\n",
    "CPU\n",
    "\n",
    "### Dataset Information\n",
    "\n",
    "| Dataset     | Number of Rows |\n",
    "|-------------|----------------|\n",
    "| Train       | 85             |\n",
    "| Test        | 15             |\n",
    "| Validation  | 15             |\n",
    "\n",
    "\n",
    "The tables above summarizes the evaluation metrics for the Scratch BERT model-\n",
    "\n",
    "- The BERT Scratch model demonstrates high losses during training and validation on the WikiMedical Sentence Similarity dataset, indicating potential issues with model convergence or architecture. \n",
    "\n",
    "\n",
    "- It achieved a precision of 0.2178, recall of 0.4667, and F1 score of 0.2970.  \n",
    "\n",
    "Further investigation is warranted to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence BERT Model Performance on  `snli` and `mnli` Dataset by using BERT Scratch Pre-trained Model\n",
    "\n",
    "- Training , Validation & Test Loss\n",
    "\n",
    "| Metric              | Value   |\n",
    "|---------------------|---------|\n",
    "| Average Training Loss | 1.1655  |\n",
    "| Average Validation Loss | 1.0398  |\n",
    "| Average Test Loss   | 1.1250   |\n",
    "\n",
    "- Precision, Recall & F1-Score\n",
    "\n",
    "| Metric    | Value   |\n",
    "|-----------|---------|\n",
    "| Precision | 0.1111  |\n",
    "| Recall    | 0.3333  |\n",
    "| F1 Score  | 0.1667  |\n",
    "\n",
    "\n",
    "\n",
    "### Device\n",
    "CPU\n",
    "\n",
    "### Dataset Information\n",
    "\n",
    "| Dataset     | Number of Rows |\n",
    "|-------------|----------------|\n",
    "| Train       | 85             |\n",
    "| Test        | 15             |\n",
    "| Validation  | 15             |\n",
    "\n",
    "\n",
    "The tables above summarizes the evaluation metrics for the Sentence BERT model-  \n",
    "\n",
    "- The Sentence BERT model achieves lower training, validation, and test losses compared to the BERT Scratch model on the WikiMedical Sentence Similarity dataset, indicating better performance and generalization capabilities.\n",
    "- It achieved a precision of 0.1111, recall of 0.3333, and F1 score of 0.1667. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison of Cosine Simiarity Performance on  `snli` and `mnli` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proess_text(sentence, tokenizer, vocab, max_seq_length):\n",
    "    tokens = tokenizer(re.sub(\"[.,!?\\\\-]\", '', sentence.lower()))\n",
    "    input_ids = [vocab['[CLS]']] + [vocab[token] for token in tokens] + [vocab['[SEP]']]\n",
    "    n_pad = max_seq_length - len(input_ids)\n",
    "    attention_mask = ([1] * len(input_ids)) + ([0] * n_pad)\n",
    "    input_ids = input_ids + ([0] * n_pad)\n",
    "\n",
    "    return {'input_ids': torch.LongTensor(input_ids).reshape(1, -1),\n",
    "            'attention_mask': torch.LongTensor(attention_mask).reshape(1, -1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mean pooling function\n",
    "def mean_pool(token_embeds, attention_mask):\n",
    "    # reshape attention_mask to cover 768-dimension embeddings\n",
    "    in_mask = attention_mask.unsqueeze(-1).expand(\n",
    "        token_embeds.size()\n",
    "    ).float()\n",
    "    # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
    "    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(\n",
    "        in_mask.sum(1), min=1e-9\n",
    "    )\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_len = 512\n",
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two tensors.\n",
    "    \"\"\"\n",
    "    dot_product = (u * v).sum()\n",
    "    norm_u = u.norm(2)\n",
    "    norm_v = v.norm(2)\n",
    "    similarity = dot_product / (norm_u * norm_v)\n",
    "    return similarity.item()\n",
    "\n",
    "def calculate_average_cosine_similarity(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Calculate the average cosine similarity between the sentence embeddings\n",
    "    of pairs in the dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    similarities = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc='Calculating Similarity', leave=False):\n",
    "            inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "            inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "            attention_a = batch['premise_attention_mask'].to(device)\n",
    "            attention_b = batch['hypothesis_attention_mask'].to(device)\n",
    "            segment_ids = torch.zeros(inputs_ids_a.size(0), max_len, dtype=torch.int32).to(device)\n",
    "\n",
    "            u_last_hidden_state = model.get_last_hidden_state(inputs_ids_a, segment_ids)\n",
    "            v_last_hidden_state = model.get_last_hidden_state(inputs_ids_b, segment_ids)\n",
    "\n",
    "            u_mean_pool = mean_pool(u_last_hidden_state, attention_a)\n",
    "            v_mean_pool = mean_pool(v_last_hidden_state, attention_b)\n",
    "\n",
    "            similarity = cosine_similarity(u_mean_pool, v_mean_pool)\n",
    "            similarities.append(similarity)\n",
    "\n",
    "    average_similarity = np.mean(similarities)\n",
    "    return average_similarity\n",
    "\n",
    "def calculate_average_cosine_similarity_st(model, data_loader):\n",
    "    \"\"\"\n",
    "    Calculate the average cosine similarity between the sentence embeddings\n",
    "    of pairs in the dataset using the SentenceTransformer model.\n",
    "    \"\"\"\n",
    "    model.eval()  # Put the model in evaluation mode\n",
    "    similarities = []\n",
    "    \n",
    "    # No need to manually handle devices as SentenceTransformer takes care of it\n",
    "    for batch in tqdm(data_loader, desc='Calculating Similarity', leave=False):\n",
    "        sentences_a = batch['premise']\n",
    "        sentences_b = batch['hypothesis']\n",
    "        \n",
    "        # Encode the batches of sentences to get their embeddings\n",
    "        embeddings_a = model.encode(sentences_a, convert_to_tensor=True)\n",
    "        embeddings_b = model.encode(sentences_b, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute the cosine similarity for each pair in the batch\n",
    "        for u, v in zip(embeddings_a, embeddings_b):\n",
    "            similarity = np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "            similarities.append(similarity)\n",
    "\n",
    "    # Calculate the average cosine similarity across all pairs\n",
    "    average_similarity = np.mean(similarities)\n",
    "    return average_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average Cosine Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERT Scratch</td>\n",
       "      <td>0.928735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence BERT</td>\n",
       "      <td>0.925694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pretrained BERT</td>\n",
       "      <td>0.593221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model  Average Cosine Similarity\n",
       "0     BERT Scratch                   0.928735\n",
       "1    Sentence BERT                   0.925694\n",
       "2  Pretrained BERT                   0.593221"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the average cosine similarity for each model\n",
    "average_similarity_Bert_Scratch = calculate_average_cosine_similarity(model_scratch, test_dataloader, device)\n",
    "average_similarity_Sentence_Bert = calculate_average_cosine_similarity(model_S_Bert, test_dataloader, device)\n",
    "average_similarity_S_BERT = calculate_average_cosine_similarity_st(model_pretrained_S_BERT, test_dataloader)\n",
    "\n",
    "# Create a DataFrame to display the results in tabular format\n",
    "data = {\n",
    "    'Model': ['BERT Scratch', 'Sentence BERT', 'Pretrained BERT'],\n",
    "    'Average Cosine Similarity': [average_similarity_Bert_Scratch, average_similarity_Sentence_Bert, average_similarity_S_BERT]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below presents a comparison of the average cosine similarity scores achieved by three different models on the `snli` and `mnli` test datasets:\n",
    "\n",
    "| Model            | Average Cosine Similarity |\n",
    "|------------------|---------------------------|\n",
    "| BERT Scratch     | 0.928735                  |\n",
    "| Sentence BERT    | 0.925694                  |\n",
    "| Pretrained BERT  | 0.593221                  |\n",
    "\n",
    "- **BERT Scratch** and **Sentence BERT** models show highly competitive performance, with cosine similarity scores close to 0.93, indicating strong capabilities in capturing semantic similarities between sentences.\n",
    "- **Pretrained BERT**, on the other hand, demonstrates a significantly lower average cosine similarity score, which suggests that, without further fine-tuning specific to the sentence similarity task, pretrained models may not perform as effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison of Spearmanr Correlation Performance on  `snli` and `mnli` Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Function to calculate Spearman correlation\n",
    "def calculate_spearman_correlation(similarity_scores, labels):\n",
    "    # Calculate Spearman correlation\n",
    "    correlation, p_value = spearmanr(similarity_scores, labels)\n",
    "    return correlation\n",
    "\n",
    "# Function to compute similarity scores for all models\n",
    "# Function to compute similarity scores for all models\n",
    "# Function to compute similarity scores for all models\n",
    "def cosine_similarity_spearmanr(u, v):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two tensors.\n",
    "    \"\"\"\n",
    "    dot_product = (u * v).sum(dim=-1)\n",
    "    norm_u = u.norm(2, dim=-1)\n",
    "    norm_v = v.norm(2, dim=-1)\n",
    "    similarity = dot_product / (norm_u * norm_v)\n",
    "    return similarity.numpy()  # Convert to numpy array\n",
    "\n",
    "def compute_similarity_scores(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    similarity_scores = []\n",
    "    \n",
    "    # Iterate through the data loader\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc='Calculating Similarity', leave=False):\n",
    "            inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "            inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "            attention_a = batch['premise_attention_mask'].to(device)\n",
    "            attention_b = batch['hypothesis_attention_mask'].to(device)\n",
    "            segment_ids = torch.zeros(inputs_ids_a.size(0), max_len, dtype=torch.int32).to(device)\n",
    "\n",
    "            u_last_hidden_state = model.get_last_hidden_state(inputs_ids_a, segment_ids)\n",
    "            v_last_hidden_state = model.get_last_hidden_state(inputs_ids_b, segment_ids)\n",
    "\n",
    "            u_mean_pool = mean_pool(u_last_hidden_state, attention_a)\n",
    "            v_mean_pool = mean_pool(v_last_hidden_state, attention_b)\n",
    "\n",
    "            similarity = cosine_similarity_spearmanr(u_mean_pool, v_mean_pool)\n",
    "            similarity_scores.extend(similarity)  # Remove .cpu() method call\n",
    "\n",
    "    return similarity_scores\n",
    "\n",
    "\n",
    "# Function to compute similarity scores using SentenceTransformer\n",
    "def compute_similarity_scores_st(model, data_loader):\n",
    "    similarity_scores = []\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Iterate through the data loader\n",
    "    for batch in tqdm(data_loader, desc='Calculating Similarity', leave=False):\n",
    "        premise_sentences = batch['premise']\n",
    "        hypothesis_sentences = batch['hypothesis']\n",
    "        \n",
    "        # Encode premise and hypothesis sentences\n",
    "        premise_embeddings = model.encode(premise_sentences, convert_to_tensor=True)\n",
    "        hypothesis_embeddings = model.encode(hypothesis_sentences, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute cosine similarity for each pair of embeddings\n",
    "        for i in range(len(premise_embeddings)):\n",
    "            u = premise_embeddings[i]\n",
    "            v = hypothesis_embeddings[i]\n",
    "            similarity = np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "            similarity_scores.append(similarity)\n",
    "\n",
    "    return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Spearman Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERT Scratch</td>\n",
       "      <td>0.083205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence BERT</td>\n",
       "      <td>-0.239710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pretrained Sentence BERT</td>\n",
       "      <td>-0.604227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Spearman Correlation\n",
       "0              BERT Scratch              0.083205\n",
       "1             Sentence BERT             -0.239710\n",
       "2  Pretrained Sentence BERT             -0.604227"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute similarity scores for each model\n",
    "similarity_scores_Bert_Scratch = compute_similarity_scores(model_scratch, test_dataloader, device)\n",
    "similarity_scores_Sentence_Bert = compute_similarity_scores(model_S_Bert, test_dataloader, device)\n",
    "# Compute similarity scores using SentenceTransformer\n",
    "similarity_scores_pretrained_S_BERT = compute_similarity_scores_st(model_pretrained_S_BERT, test_dataloader)\n",
    "\n",
    "\n",
    "# Extract ground truth labels from the test dataset\n",
    "ground_truth_labels = [label.item() for batch in test_dataloader for label in batch['labels']]\n",
    "\n",
    "# Calculate Spearman correlation for each model\n",
    "spearman_corr_Bert_Scratch = calculate_spearman_correlation(similarity_scores_Bert_Scratch, ground_truth_labels)\n",
    "spearman_corr_Sentence_Bert = calculate_spearman_correlation(similarity_scores_Sentence_Bert, ground_truth_labels)\n",
    "# Calculate Spearman correlation\n",
    "spearman_corr_pretrained_S_BERT = spearmanr(similarity_scores_pretrained_S_BERT, ground_truth_labels).correlation\n",
    "\n",
    "# # Display the Spearman correlations\n",
    "# print(\"Spearman Correlation for BERT Scratch Model:\", spearman_corr_Bert_Scratch)\n",
    "# print(\"Spearman Correlation for Sentence BERT Model:\", spearman_corr_Sentence_Bert)\n",
    "# # Display the Spearman correlation\n",
    "# print(\"Spearman Correlation for Pretrained Sentence BERT Model:\", spearman_corr_pretrained_S_BERT)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['BERT Scratch', 'Sentence BERT', 'Pretrained Sentence BERT'],\n",
    "    'Spearman Correlation': [spearman_corr_Bert_Scratch, spearman_corr_Sentence_Bert, spearman_corr_pretrained_S_BERT]\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "display(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This table presents the Spearman correlation coefficients for the models on the `snli` and `mnli` test dataset, reflecting their alignment with human judgments of sentence similarity:\n",
    "\n",
    "| Model                     | Spearman Correlation |\n",
    "|---------------------------|----------------------|\n",
    "| BERT Scratch              | 0.083205             |\n",
    "| Sentence BERT             | -0.239710            |\n",
    "| Pretrained Sentence BERT  | -0.604227            |\n",
    "\n",
    "- **BERT Scratch** achieves a slight positive Spearman correlation, suggesting some level of agreement with human judgments of similarity.\n",
    "- **Sentence BERT** and **Pretrained Sentence BERT** exhibit negative Spearman correlations, indicating a divergence from human similarity judgments, with Pretrained Sentence BERT showing the most significant deviation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Deployment Recommendation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Overview\n",
    "A comparison of BERT Scratch, Sentence BERT, and Pretrained Models on various performance metrics has been conducted to determine the best model for deployment in a medical text similarity context.\n",
    "\n",
    "### Performance Summary\n",
    "- **BERT Scratch Model**: High training/validation losses and low precision/F1 score indicate convergence issues and inaccuracies in similarity detection.\n",
    "- **Sentence BERT Model**: Exhibits lower losses and better overall performance, suggesting stronger generalization capabilities.\n",
    "- **Pretrained Models**: Show limitations in domain-specific semantic capture, as evidenced by lower cosine similarity scores and negative Spearman correlations.\n",
    "\n",
    "### Deployment Considerations\n",
    "1. **Performance vs. Requirements**: Sentence BERT's balanced performance makes it a suitable candidate for scenarios prioritizing accuracy in medical texts.\n",
    "2. **Dataset Specificity**: Models trained on domain-specific datasets are preferred for similar deployment contexts.\n",
    "3. **Computational Efficiency**: Consider computational demands in relation to the deployment environment.\n",
    "4. **Integration and Maintenance**: Sentence BERT offers customization benefits but may require more maintenance effort.\n",
    "\n",
    "### Recommendation\n",
    "For deployments focusing on medical domain accuracy, **Sentence BERT** is recommended due to its better loss metrics and generalization. Continuous performance monitoring and iterative improvements post-deployment are advised to ensure sustained effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Detailed Evaluation and Analysis Report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This report provides a comprehensive analysis of the Sentence Transformer model's performance, comparing it with BERT Scratch and other pre-trained models. We also explore the impact of hyperparameter choices and propose potential improvements.\n",
    "\n",
    "### 1. Detailed Evaluation of the Sentence Transformer Model\n",
    "\n",
    "#### Types of Sentences and Relevance\n",
    "\n",
    "Our models were evaluated on diverse datasets including WikiMedical Sentence Similarity, `snli`, and `mnli`. These datasets encompass a wide range of sentence structures and contexts, crucial for evaluating the model's performance across different linguistic and domain-specific scenarios.\n",
    "\n",
    "#### Evaluation Metrics\n",
    "\n",
    "- **Losses**: We've observed the average training, validation, and test losses as primary metrics.\n",
    "- **Cosine Similarity**: This metric provided insights into the model's ability to capture semantic similarity.\n",
    "- **Spearman’s Correlation**: Used to assess the alignment of model predictions with human judgment.\n",
    "- **Precision, Recall, F1 Score**: These additional metrics should be considered for a rounded evaluation, especially for classification tasks inherent in `snli` and `mnli`.\n",
    "\n",
    "### 2. Comparison with Pre-trained Models\n",
    "\n",
    "Our models demonstrate varying degrees of performance, with tailored training on specific datasets enhancing their ability to capture nuances compared to generic pre-trained models.\n",
    "\n",
    "### Observations\n",
    "\n",
    "- **Domain Specificity**: The fine-tuning process on specialized datasets has shown significant improvements in model performance, underscoring the importance of domain-specific adaptation.\n",
    "- **Performance Discrepancies**: The lower performance of generic pre-trained models on these tasks highlights the need for task-specific fine-tuning and adaptation.\n",
    "\n",
    "### 3. Impact of Hyperparameter Choices\n",
    "\n",
    "Hyperparameter tuning is critical for optimizing model performance. Our analysis focuses on the effects of learning rate, batch size, epochs, and optimizer choice.\n",
    "\n",
    "### Findings\n",
    "\n",
    "- **Learning Rate and Batch Size**: These have a significant impact on the model's ability to converge and generalize.\n",
    "- **Epochs and Optimizers**: The number of training cycles and the choice of optimization algorithm can drastically affect the outcome, balancing between underfitting and overfitting.\n",
    "\n",
    "### Strategies\n",
    "\n",
    "- Implementing **ablation studies** to isolate the impact of individual hyperparameters can provide deeper insights into optimal configurations.\n",
    "\n",
    "### 4. Limitations, Challenges, and Improvements\n",
    "\n",
    "#### Limitations and Challenges\n",
    "\n",
    "- **Data Size**: The relatively small size of our datasets poses challenges for deep learning models, which typically require large amounts of data.\n",
    "- **Overfitting Risks**: Strategies to mitigate overfitting, such as regularization and data augmentation, are crucial given the dataset sizes.\n",
    "\n",
    "#### Proposed Improvements\n",
    "\n",
    "- **Data Enhancement and Augmentation**: Employing techniques to synthetically expand our datasets can help improve model robustness.\n",
    "- **Transfer Learning and Domain Adaptation**: Further exploration into sophisticated pre-trained models and domain adaptation strategies is needed.\n",
    "- **Hyperparameter Optimization**: Automated techniques like grid search and Bayesian optimization could systematically identify optimal model configurations.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Our analysis has revealed key insights into the performance of Sentence Transformer models, highlighting the importance of domain-specific training, comprehensive evaluation metrics, and the critical role of hyperparameter tuning. By addressing the identified limitations and implementing the suggested improvements, we can enhance our models' capabilities, making significant strides in the field of NLP and its application to specialized domains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
